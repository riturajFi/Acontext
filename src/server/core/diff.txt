diff --git a/src/server/core/acontext_core/schema/config.py b/src/server/core/acontext_core/schema/config.py
index d52f846..dada38d 100644
--- a/src/server/core/acontext_core/schema/config.py
+++ b/src/server/core/acontext_core/schema/config.py
@@ -50,6 +50,9 @@ class CoreConfig(BaseModel):
     session_message_session_lock_wait_seconds: int = 1
     session_message_processing_timeout_seconds: int = 60
     space_task_sop_lock_wait_seconds: int = 1
+    space_task_sop_batch_wait_seconds: int = 2
+    space_task_sop_batch_max_size: int = 16
+    space_task_sop_buffer_ttl_seconds: int = 30 * 60
 
     # MQ Configuration
     mq_url: str = "amqp://acontext:helloworld@127.0.0.1:15672/"
diff --git a/src/server/core/acontext_core/service/controller/space_sop.py b/src/server/core/acontext_core/service/controller/space_sop.py
index 3f64296..81d0a3c 100644
--- a/src/server/core/acontext_core/service/controller/space_sop.py
+++ b/src/server/core/acontext_core/service/controller/space_sop.py
@@ -5,23 +5,21 @@ from ...env import LOG
 from ...schema.config import ProjectConfig
 
 
-async def process_sop_complete(
+async def process_sop_complete_batch(
     project_config: ProjectConfig,
     project_id: asUUID,
     space_id: asUUID,
-    task_id: asUUID,
-    sop_data: SOPData,
+    task_ids: list[asUUID],
+    sop_datas: list[SOPData],
 ):
     """
-    Process SOP completion and trigger construct agent
+    Process SOP completions and trigger construct agent.
     """
-    LOG.info(f"Processing SOP completion for task {task_id}")
-    # Call construct agent
     construct_result = await SC.space_construct_agent_curd(
         project_id,
         space_id,
-        [task_id],
-        [sop_data],
+        task_ids,
+        sop_datas,
         max_iterations=project_config.default_space_construct_agent_max_iterations,
     )
 
@@ -32,3 +30,23 @@ async def process_sop_complete(
         LOG.error(f"Construct agent failed: {construct_result}")
 
     return construct_result
+
+
+async def process_sop_complete(
+    project_config: ProjectConfig,
+    project_id: asUUID,
+    space_id: asUUID,
+    task_id: asUUID,
+    sop_data: SOPData,
+):
+    """
+    Process SOP completion and trigger construct agent
+    """
+    LOG.info(f"Processing SOP completion for task {task_id}")
+    return await process_sop_complete_batch(
+        project_config,
+        project_id,
+        space_id,
+        [task_id],
+        [sop_data],
+    )
diff --git a/src/server/core/acontext_core/service/space_receive_sop.py b/src/server/core/acontext_core/service/space_receive_sop.py
index e17b7c3..2e29caa 100644
--- a/src/server/core/acontext_core/service/space_receive_sop.py
+++ b/src/server/core/acontext_core/service/space_receive_sop.py
@@ -1,3 +1,5 @@
+import asyncio
+
 from ..env import LOG, DEFAULT_CORE_CONFIG
 from ..infra.db import DB_CLIENT
 from ..infra.async_mq import (
@@ -13,6 +15,7 @@ from .data import project as PD
 from .data import task as TD
 from .data import session as SD
 from .controller import space_sop as SSC
+from . import space_sop_buffer as SSB
 from .utils import check_redis_lock_or_set, release_redis_lock
 
 register_consumer(
@@ -43,52 +46,377 @@ async def space_sop_complete_task(body: SOPComplete, message: Message):
     if body.task_id is None:
         LOG.error("Task ID is required for SOP complete")
         return
+
+    buffered = await SSB.push_sop_buffer(body)
+    if not buffered:
+        LOG.warning(
+            f"Failed to buffer SOPComplete for space {body.space_id} (task {body.task_id})"
+        )
+
     _lock_key = f"{RK.space_task_sop_complete}.{body.space_id}"
-    _l = await check_redis_lock_or_set(body.project_id, _lock_key)
+    try:
+        _l = await check_redis_lock_or_set(body.project_id, _lock_key)
+    except Exception as e:
+        LOG.error(
+            f"Failed to acquire SOP space lock due to Redis error (space={body.space_id}, task={body.task_id}): {e}"
+        )
+        try:
+            await MQ_CLIENT.publish(
+                exchange_name=EX.space_task,
+                routing_key=RK.space_task_sop_complete_retry,
+                body=body.model_dump_json(),
+            )
+        except Exception as publish_e:
+            LOG.error(
+                f"Failed to publish SOPComplete retry after lock acquisition error "
+                f"(space={body.space_id}, task={body.task_id}): {publish_e}"
+            )
+        return
     if not _l:
         LOG.debug(
             f"Current Space {body.space_id} is locked. "
             f"wait {DEFAULT_CORE_CONFIG.space_task_sop_lock_wait_seconds} seconds for next resend. "
         )
-        await MQ_CLIENT.publish(
-            exchange_name=EX.space_task,
-            routing_key=RK.space_task_sop_complete_retry,
-            body=body.model_dump_json(),
-        )
+        try:
+            await MQ_CLIENT.publish(
+                exchange_name=EX.space_task,
+                routing_key=RK.space_task_sop_complete_retry,
+                body=body.model_dump_json(),
+            )
+        except Exception as publish_e:
+            LOG.error(
+                f"Failed to publish SOPComplete retry while space locked "
+                f"(space={body.space_id}, task={body.task_id}): {publish_e}"
+            )
         return
     LOG.info(f"Lock Space {body.space_id} for SOP complete task")
     try:
-        async with DB_CLIENT.get_session_context() as db_session:
-            # First get the task to find its session_id
-            r = await TD.fetch_task(db_session, body.task_id)
-            if not r.ok():
-                LOG.error(f"Task not found: {body.task_id}")
-                return
-            task_data, _ = r.unpack()
-
-            # Verify session exists and has space
-            r = await SD.fetch_session(db_session, task_data.session_id)
-            if not r.ok():
-                LOG.error(f"Session not found for task {body.task_id}")
-                return
-            session_data, _ = r.unpack()
-            if session_data.space_id is None:
-                LOG.info(f"Session {task_data.session_id} has no linked space")
-                return
-
-            # Get project config
-            r = await PD.get_project_config(db_session, body.project_id)
-            project_config, eil = r.unpack()
-            if eil:
-                LOG.error(f"Project config not found for project {body.project_id}")
-                return
-
-        # Call controller to process SOP completion
-        await SSC.process_sop_complete(
-            project_config, body.project_id, body.space_id, body.task_id, body.sop_data
+        popped_entries: list[str] = []
+        current_entry_json = body.model_dump_json()
+        batch_log_prefix = (
+            f"SOP_BATCH space={body.space_id} project={body.project_id}"
+        )
+
+        try:
+            await SSB.remove_sop_buffer_entry(
+                body.project_id, body.space_id, current_entry_json, count=0
+            )
+        except Exception as e:
+            LOG.warning(
+                f"Failed to remove current SOPComplete from buffer (space={body.space_id}, task={body.task_id}): {e}"
+            )
+
+        batch_wait_seconds = DEFAULT_CORE_CONFIG.space_task_sop_batch_wait_seconds
+        if batch_wait_seconds > 0:
+            await asyncio.sleep(batch_wait_seconds)
+
+        max_batch_size = DEFAULT_CORE_CONFIG.space_task_sop_batch_max_size
+        if max_batch_size <= 0:
+            max_batch_size = 1
+            LOG.warning(
+                "Invalid space_task_sop_batch_max_size, using default batch size: 1"
+            )
+        pop_size = max(max_batch_size - 1, 0)
+        if pop_size > 0:
+            popped_entries = await SSB.pop_sop_buffer_batch(
+                body.project_id, body.space_id, pop_size
+            )
+
+        skip_reasons: dict[str, int] = {}
+
+        def _skip(reason: str):
+            skip_reasons[reason] = skip_reasons.get(reason, 0) + 1
+
+        parsed_items: list[tuple[str, SOPComplete]] = [(current_entry_json, body)]
+        for entry_json in popped_entries:
+            item = SSB.parse_sop_buffer_entry(entry_json)
+            if item is None:
+                _skip("invalid_payload")
+                continue
+            if item.project_id != body.project_id or item.space_id != body.space_id:
+                _skip("project_or_space_mismatch")
+                continue
+            if item.task_id is None:
+                _skip("missing_task_id")
+                continue
+            parsed_items.append((entry_json, item))
+
+        LOG.info(
+            f"{batch_log_prefix} drained popped={len(popped_entries) + 1} "
+            f"parsed={len(parsed_items)} skip={sum(skip_reasons.values())} "
+            f"skip_reasons={skip_reasons}"
+        )
+
+        seen_task_ids: set = set()
+        unique_items: list[tuple[str, SOPComplete]] = []
+        for entry_json, item in parsed_items:
+            if item.task_id in seen_task_ids:
+                _skip("duplicate_task_id")
+                continue
+            seen_task_ids.add(item.task_id)
+            unique_items.append((entry_json, item))
+
+        project_config = None
+        processable_items: list[tuple[str, SOPComplete]] = []
+        current_is_processable = False
+        try:
+            async with DB_CLIENT.get_session_context() as db_session:
+                r = await PD.get_project_config(db_session, body.project_id)
+                project_config, eil = r.unpack()
+                if eil:
+                    for _entry_json, _item in unique_items:
+                        _skip("project_not_found")
+                else:
+                    for entry_json, item in unique_items:
+                        r = await TD.fetch_task(db_session, item.task_id)
+                        if not r.ok():
+                            _skip("task_not_found")
+                            continue
+                        task_data, _ = r.unpack()
+                        if task_data.space_digested:
+                            _skip("task_already_space_digested")
+                            continue
+                        r = await SD.fetch_session(db_session, task_data.session_id)
+                        if not r.ok():
+                            _skip("session_not_found")
+                            continue
+                        session_data, _ = r.unpack()
+                        if session_data.space_id is None:
+                            _skip("session_has_no_space")
+                            continue
+                        if session_data.space_id != body.space_id:
+                            _skip("task_space_mismatch")
+                            continue
+                        processable_items.append((entry_json, item))
+                        if item.task_id == body.task_id:
+                            current_is_processable = True
+        except Exception as e:
+            LOG.warning(
+                f"{batch_log_prefix} retryable_failure=db_exception "
+                f"retryable_items={len(unique_items)} "
+                f"skip={sum(skip_reasons.values())} skip_reasons={skip_reasons}: {e}"
+            )
+            restored = await SSB.push_sop_buffer_entries_json(
+                body.project_id,
+                body.space_id,
+                [entry_json for entry_json, _item in unique_items],
+            )
+            if restored:
+                try:
+                    await MQ_CLIENT.publish(
+                        exchange_name=EX.space_task,
+                        routing_key=RK.space_task_sop_complete_retry,
+                        body=body.model_dump_json(),
+                    )
+                except Exception as publish_e:
+                    LOG.error(
+                        f"Failed to publish SOPComplete retry after DB failure "
+                        f"(space={body.space_id}, task={body.task_id}): {publish_e}"
+                    )
+            else:
+                for _entry_json, item in unique_items:
+                    try:
+                        await MQ_CLIENT.publish(
+                            exchange_name=EX.space_task,
+                            routing_key=RK.space_task_sop_complete_retry,
+                            body=item.model_dump_json(),
+                        )
+                    except Exception as publish_e:
+                        LOG.error(
+                            f"Failed to publish SOPComplete retry after DB failure "
+                            f"(space={body.space_id}, task={item.task_id}): {publish_e}"
+                        )
+            LOG.warning(
+                f"SOPComplete batch DB failure, scheduled retry "
+                f"(space={body.space_id}, retryable_failure={len(unique_items)}, popped={len(popped_entries) + 1}, "
+                f"skip={sum(skip_reasons.values())}, skip_reasons={skip_reasons}): {e}"
+            )
+            return
+
+        if not processable_items:
+            try:
+                await SSB.remove_sop_buffer_entry(
+                    body.project_id, body.space_id, current_entry_json, count=0
+                )
+            except Exception:
+                pass
+            LOG.info(
+                f"{batch_log_prefix} empty_after_filtering popped={len(popped_entries) + 1} "
+                f"processable=0 skip={sum(skip_reasons.values())} skip_reasons={skip_reasons}"
+            )
+            return
+
+        retry_trigger_item = processable_items[0][1]
+        try:
+            task_ids = [item.task_id for _entry_json, item in processable_items]
+            sop_datas = [item.sop_data for _entry_json, item in processable_items]
+            LOG.info(
+                f"{batch_log_prefix} processing size={len(processable_items)} "
+                f"popped={len(popped_entries) + 1} skip={sum(skip_reasons.values())} "
+                f"skip_reasons={skip_reasons}"
+            )
+            r = await SSC.process_sop_complete_batch(
+                project_config,
+                body.project_id,
+                body.space_id,
+                task_ids,
+                sop_datas,
+            )
+        except Exception as e:
+            LOG.warning(
+                f"{batch_log_prefix} retryable_failure=agent_exception "
+                f"retryable_items={len(processable_items)} "
+                f"skip={sum(skip_reasons.values())} skip_reasons={skip_reasons}: {e}"
+            )
+            if not current_is_processable:
+                try:
+                    await SSB.remove_sop_buffer_entry(
+                        body.project_id, body.space_id, current_entry_json, count=0
+                    )
+                except Exception:
+                    pass
+            restored = await SSB.push_sop_buffer_entries_json(
+                body.project_id,
+                body.space_id,
+                [entry_json for entry_json, _item in processable_items],
+            )
+            if restored:
+                try:
+                    await MQ_CLIENT.publish(
+                        exchange_name=EX.space_task,
+                        routing_key=RK.space_task_sop_complete_retry,
+                        body=retry_trigger_item.model_dump_json(),
+                    )
+                except Exception as publish_e:
+                    LOG.error(
+                        f"Failed to publish SOPComplete retry after processing exception "
+                        f"(space={body.space_id}, task={retry_trigger_item.task_id}): {publish_e}"
+                    )
+            else:
+                for _entry_json, item in processable_items:
+                    try:
+                        await MQ_CLIENT.publish(
+                            exchange_name=EX.space_task,
+                            routing_key=RK.space_task_sop_complete_retry,
+                            body=item.model_dump_json(),
+                        )
+                    except Exception as publish_e:
+                        LOG.error(
+                            f"Failed to publish SOPComplete retry after processing exception "
+                            f"(space={body.space_id}, task={item.task_id}): {publish_e}"
+                        )
+            LOG.warning(
+                f"SOPComplete batch processing exception, scheduled retry "
+                f"(space={body.space_id}, retryable_failure={len(processable_items)}, popped={len(popped_entries) + 1}, "
+                f"skip={sum(skip_reasons.values())}, skip_reasons={skip_reasons}): {e}"
+            )
+            return
+
+        if not r.ok():
+            LOG.warning(
+                f"{batch_log_prefix} retryable_failure=agent_result "
+                f"retryable_items={len(processable_items)} "
+                f"skip={sum(skip_reasons.values())} skip_reasons={skip_reasons}"
+            )
+            if not current_is_processable:
+                try:
+                    await SSB.remove_sop_buffer_entry(
+                        body.project_id, body.space_id, current_entry_json, count=0
+                    )
+                except Exception:
+                    pass
+            restored = await SSB.push_sop_buffer_entries_json(
+                body.project_id,
+                body.space_id,
+                [entry_json for entry_json, _item in processable_items],
+            )
+            if restored:
+                try:
+                    await MQ_CLIENT.publish(
+                        exchange_name=EX.space_task,
+                        routing_key=RK.space_task_sop_complete_retry,
+                        body=retry_trigger_item.model_dump_json(),
+                    )
+                except Exception as publish_e:
+                    LOG.error(
+                        f"Failed to publish SOPComplete retry after batch failure "
+                        f"(space={body.space_id}, task={retry_trigger_item.task_id}): {publish_e}"
+                    )
+            else:
+                for _entry_json, item in processable_items:
+                    try:
+                        await MQ_CLIENT.publish(
+                            exchange_name=EX.space_task,
+                            routing_key=RK.space_task_sop_complete_retry,
+                            body=item.model_dump_json(),
+                        )
+                    except Exception as publish_e:
+                        LOG.error(
+                            f"Failed to publish SOPComplete retry after batch failure "
+                            f"(space={body.space_id}, task={item.task_id}): {publish_e}"
+                        )
+            LOG.warning(
+                f"SOPComplete batch retryable failure, scheduled retry "
+                f"(space={body.space_id}, retryable_failure={len(processable_items)}, popped={len(popped_entries) + 1}, "
+                f"skip={sum(skip_reasons.values())}, skip_reasons={skip_reasons})"
+            )
+            return
+
+        try:
+            await SSB.remove_sop_buffer_entry(
+                body.project_id, body.space_id, current_entry_json, count=0
+            )
+        except Exception:
+            pass
+        LOG.info(
+            f"{batch_log_prefix} done success={len(processable_items)} "
+            f"popped={len(popped_entries) + 1} skip={sum(skip_reasons.values())} "
+            f"skip_reasons={skip_reasons}"
         )
 
     except Exception as e:
-        LOG.error(f"Error in space_sop_complete_task: {e}")
+        LOG.warning(f"{batch_log_prefix} retryable_failure=unknown_exception: {e}")
+        restore_entries = [body.model_dump_json(), *popped_entries]
+        restored = await SSB.push_sop_buffer_entries_json(
+            body.project_id,
+            body.space_id,
+            restore_entries,
+        )
+        if restored:
+            try:
+                await MQ_CLIENT.publish(
+                    exchange_name=EX.space_task,
+                    routing_key=RK.space_task_sop_complete_retry,
+                    body=body.model_dump_json(),
+                )
+            except Exception as publish_e:
+                LOG.error(
+                    f"Failed to publish SOPComplete retry after unexpected error "
+                    f"(space={body.space_id}, task={body.task_id}): {publish_e}"
+                )
+        else:
+            for entry_json in restore_entries:
+                item = SSB.parse_sop_buffer_entry(entry_json)
+                if item is None:
+                    continue
+                try:
+                    await MQ_CLIENT.publish(
+                        exchange_name=EX.space_task,
+                        routing_key=RK.space_task_sop_complete_retry,
+                        body=item.model_dump_json(),
+                    )
+                except Exception as publish_e:
+                    LOG.error(
+                        f"Failed to publish SOPComplete retry after unexpected error "
+                        f"(space={body.space_id}, task={item.task_id}): {publish_e}"
+                    )
+        LOG.error(
+            f"Error in space_sop_complete_task, scheduled retry (space={body.space_id}, task={body.task_id}): {e}"
+        )
     finally:
-        await release_redis_lock(body.project_id, _lock_key)
+        try:
+            await release_redis_lock(body.project_id, _lock_key)
+        except Exception as e:
+            LOG.error(
+                f"Failed to release SOP space lock (space={body.space_id}, task={body.task_id}): {e}"
+            )
diff --git a/src/server/core/acontext_core/service/space_sop_buffer.py b/src/server/core/acontext_core/service/space_sop_buffer.py
new file mode 100644
index 0000000..09da47c
--- /dev/null
+++ b/src/server/core/acontext_core/service/space_sop_buffer.py
@@ -0,0 +1,155 @@
+from ..env import DEFAULT_CORE_CONFIG, LOG
+from ..infra.redis import REDIS_CLIENT
+from ..schema.mq.sop import SOPComplete
+from ..schema.utils import asUUID
+
+
+SPACE_SOP_BUFFER_KEY_PREFIX = "space_sop_buffer"
+
+
+def space_sop_buffer_key(project_id: asUUID, space_id: asUUID) -> str:
+    """
+    Per-space SOP buffer key.
+
+    Format: space_sop_buffer.<project_id>.<space_id>
+    """
+    return f"{SPACE_SOP_BUFFER_KEY_PREFIX}.{project_id}.{space_id}"
+
+
+async def push_sop_complete_to_buffer(body: SOPComplete) -> bool:
+    """
+    Append SOPComplete payload JSON to the per-space Redis list and refresh TTL.
+
+    Returns:
+        True if pushed successfully, False otherwise.
+    """
+    ttl_seconds = DEFAULT_CORE_CONFIG.space_task_sop_buffer_ttl_seconds
+    if ttl_seconds <= 0:
+        ttl_seconds = 30 * 60
+        LOG.warning(
+            "Invalid space_task_sop_buffer_ttl_seconds, using default TTL: "
+            f"{ttl_seconds} seconds"
+        )
+
+    buffer_key = space_sop_buffer_key(body.project_id, body.space_id)
+    payload_json = body.model_dump_json()
+
+    try:
+        async with REDIS_CLIENT.get_client_context() as client:
+            pipe = client.pipeline(transaction=True)
+            pipe.rpush(buffer_key, payload_json)
+            pipe.expire(buffer_key, ttl_seconds)
+            await pipe.execute()
+        return True
+    except Exception as e:
+        LOG.error(f"Failed to push SOPComplete to Redis buffer {buffer_key}: {e}")
+        return False
+
+
+async def push_sop_buffer(body: SOPComplete) -> bool:
+    """
+    Backwards/ergonomic alias: push SOPComplete payload into the per-space buffer.
+    """
+    return await push_sop_complete_to_buffer(body)
+
+
+async def push_sop_buffer_entries_json(
+    project_id: asUUID, space_id: asUUID, entries_json: list[str]
+) -> bool:
+    """
+    Append raw JSON payload(s) to the per-space SOP buffer and refresh TTL.
+
+    This is primarily used to restore popped entries on retryable failures.
+    """
+    if not entries_json:
+        return True
+
+    ttl_seconds = DEFAULT_CORE_CONFIG.space_task_sop_buffer_ttl_seconds
+    if ttl_seconds <= 0:
+        ttl_seconds = 30 * 60
+        LOG.warning(
+            "Invalid space_task_sop_buffer_ttl_seconds, using default TTL: "
+            f"{ttl_seconds} seconds"
+        )
+
+    buffer_key = space_sop_buffer_key(project_id, space_id)
+    try:
+        async with REDIS_CLIENT.get_client_context() as client:
+            pipe = client.pipeline(transaction=True)
+            pipe.rpush(buffer_key, *entries_json)
+            pipe.expire(buffer_key, ttl_seconds)
+            await pipe.execute()
+        return True
+    except Exception as e:
+        LOG.error(f"Failed to restore SOP buffer entries to {buffer_key}: {e}")
+        return False
+
+
+_POP_BATCH_LUA = """
+local key = KEYS[1]
+local count = tonumber(ARGV[1])
+if not count or count <= 0 then
+  return {}
+end
+local items = redis.call('LRANGE', key, 0, count - 1)
+if #items > 0 then
+  redis.call('LTRIM', key, #items, -1)
+end
+return items
+"""
+
+
+async def pop_sop_buffer_batch(
+    project_id: asUUID, space_id: asUUID, max_n: int
+) -> list[str]:
+    """
+    Pop up to `max_n` items from the per-space Redis list atomically.
+
+    This is intended to be called only by the worker holding the space lock.
+    """
+    if max_n <= 0:
+        return []
+
+    buffer_key = space_sop_buffer_key(project_id, space_id)
+    async with REDIS_CLIENT.get_client_context() as client:
+        items = await client.eval(_POP_BATCH_LUA, 1, buffer_key, max_n)
+
+    if items is None:
+        return []
+    if isinstance(items, str):
+        return [items]
+    return list(items)
+
+
+async def remove_sop_buffer_entry(
+    project_id: asUUID, space_id: asUUID, entry_json: str, count: int = 0
+) -> bool:
+    """
+    Remove matching entry JSON string(s) from the per-space buffer list.
+
+    Args:
+        count: Redis LREM count semantics. Use 0 to remove all occurrences.
+    """
+    if not entry_json:
+        return True
+
+    buffer_key = space_sop_buffer_key(project_id, space_id)
+    try:
+        async with REDIS_CLIENT.get_client_context() as client:
+            await client.lrem(buffer_key, count, entry_json)
+        return True
+    except Exception as e:
+        LOG.error(f"Failed to remove SOP buffer entry from {buffer_key}: {e}")
+        return False
+
+
+def parse_sop_buffer_entry(entry_json: str) -> SOPComplete | None:
+    """
+    Parse a Redis buffer entry into SOPComplete.
+
+    Buffer entries are expected to be `SOPComplete.model_dump_json()` strings.
+    """
+    try:
+        return SOPComplete.model_validate_json(entry_json)
+    except Exception:
+        return None
